\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{multicol}
\usepackage{xspace}
\usepackage{stmaryrd}
\usepackage{graphicx}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\newcommand{\var}[1]{\ensuremath{\textit{#1}}}
\newcommand{\interpret}[1]{\llbracket #1 \rrbracket}

\newcommand{\emptylist}{\ensuremath{[\:]}}

\newcommand{\pnot}[1]{\bar{#1}}
\newcommand{\attrsym}{\ensuremath{\textit{Attr}}}
\newcommand{\attr}[3][]{\ensuremath{\attrsym^{#1}_{#2}(#3)}}
\newcommand{\pred}{\ensuremath{\textit{pred}}}

\title{Symbolic Reachability using LDDs}
\author{Wieger Wesselink and Maurice Laveaux}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
In this document we describe a symbolic reachability algorithm that uses list decision diagrams (LDDs) to store states and transitions.
It is based on the work in \cite{MeijerBlomPol2008} and \cite{Meijer2019}.

\subsection{Definitions}
Let $S$ be a set of states, and $R \subseteq S \times S$ a transition relation. There is a transition from $x$ to $y$ if $(x,y) \in R$.

\noindent
We assume that the set of states $S$ is a Cartesian product

\[
S = S_1 \times \ldots \times S_m
\]

\noindent
In other words, states are vectors of $m$ elements. \\

\begin{definition}
The domain of a relation $R$ is defined as
\[
\textsf{domain}(R) = \{ x \in S \mid \exists y \in S:  (x,y) \in R \}
\]
\end{definition}

\begin{definition}
The function \textsf{next} returns the successors of an element $x \in S$:
\[
\textsf{next}(R, x) = \{ y \in S \mid (x,y) \in R \}
\]
It can be lifted to a subset $X \subseteq S$ using
\[
\textsf{next}(R, X) = \cup \{ \textsf{next}(R, x) \mid x \in X \}
\]
\end{definition}

\begin{definition}
The set of reachable states that can be reached from an initial state $x \in S$ is defined as
\[
    \textsf{reachable\_states}(R, x) = \{ y \in S \mid \exists n \geq 0: (x, y) \in R^n \}
\]
\end{definition}

\section{Read, write and copy parameters}

In \cite{DBLP:conf/hvc/MeijerKBP14} three types of dependencies for the parameters of a relation are distinguished: \emph{read dependence} (whether the value of a parameter influences transitions), \emph{must-write dependence} (whether a parameter
is written to), and \emph{may-write dependence} (whether a parameter may be written to, depending on the value of some other parameter). The may-write versus must-write distinction is introduced for arrays, that do not exist in mCRL2. So for our use
cases may- and must-write dependence coincide, and we will refer to it as
\emph{write dependence} instead.

\vspace{0.5cm}
\noindent
Before we formalize the notions \emph{read independent} and \emph{write independent}, it is important to understand the application that they are used for. We assume that we have a sparse relation $R$ that is defined as a set of pairs $(x,y)$ with $x,y \in S$.
Our goal is to define the notions read and write independent such that the values of read and write independent parameters are not needed for the successor computation of $R$. So for the values $x_i,y_i$ corresponding to a read independent parameter $d_i$ we do not need to store value $x_i$, and for the values corresponding to a write independent parameter we do not need to store the value $y_i$.

In order to satisfy these requirements we define a
read independent parameter as a parameter whose value is always copied ($y_i = x_i$), or mapped to a constant value ($y_i = c$). However, this is not enough for being able to discard the value of such a parameter. In addition, the corresponding transition has
to be enabled for any value in its domain. We define a write independent parameter as a parameter whose value is always copied  ($y_i = x_i$).

\begin{definition}
For a vector $x = [x_1, \ldots, x_m] \in S$ we define the following notation for updating the element at position $i$ with value $y \in S_i$:
\[
x[i=y] = [x_1, \ldots, x_{i-1}, y, x_{i+1}, \ldots, x_m]
\]
We lift this definition to a set as follows.
Let $I = \{ i_1, \ldots i_k \}$ with $1 \leq i_1 < \ldots < i_k \leq m$ be a set of parameter indices, and
$y \in S_{i_1} \times \ldots \times S_{i_k}$. Then
\[
x[I = y] = z \text{ with } z_r = 
  \left\{
    \begin{array}{ll}
         x_r & \text{if } r \notin I \\
         y_r & \text{otherwise}
    \end{array}
  \right. 
\]
\end{definition}

\begin{definition}
\[
\textsf{always\_copy}(R, i) = 
  (\forall (x, y) \in R: x_i = y_i) \land
  (\forall (x, y) \in R, d \in S_i: (x[i=d], y[i=d]) \in R)
\]
\end{definition}

\begin{definition}
\[
\textsf{always\_constant}(R, i) = 
  (\exists d \in S_i: \forall (x, y) \in R: y_i = d) \land
  (\forall (x, y) \in R, d \in S_i: (x[i=d], y) \in R)
\]
\end{definition}

\begin{definition}
\[
\textsf{read\_independent}(R, i) = 
\textsf{always\_copy}(R, i) \lor \textsf{always\_constant}(R, i)
\]
\end{definition}

\begin{definition}
\[
\textsf{write\_independent}(R, i) = 
  \forall (x, y) \in R: x_i = y_i
\]
\end{definition}

\noindent
Note that our definitions differ slightly from the ones in \cite{DBLP:conf/hvc/MeijerKBP14},
but they are equivalent. To illustrate these definitions, consider the
following example. 

\begin{example}
Let $S = \mathbb{N} \times \mathbb{N} \times \mathbb{N}$ and let $R$ be a relation on the variables $x_1$, $x_2$ and
$x_3$ defined by the statement

\[
\textsf{if } x_3 > 4 \textsf{ then begin } x_2 := 3 \textsf{ end}
\]
In this case $x_1$ is both read and write independent, $x_2$ is 
read independent, and $x_3$ is write independent. Even though the
value of $x_3$ is always copied, we do not consider it read independent. This is because for values $x_3 \leq 4$ the transition is not enabled,
and this information would be lost if we discard those values from
the transition relation. Now instead of storing the transition
$((1,2,5), (1,3,5))$, we only need to store $((5),(3))$ to be able
to derive that a transition from $(1,2,5)$ to $(1,3,5)$ is possible.
\end{example}

\noindent
Read and write parameters are defined using

\begin{definition}
\[
\textsf{read}(R, i) = 
\neg \textsf{read\_independent}(R, i)
\]
\end{definition}

\begin{definition}
\[
\textsf{write}(R, i) = \neg \textsf{write\_independent}(R, i)
\]
\end{definition}

\begin{definition}
\[
\textsf{read\_parameters}(R) = \{ i \mid \textsf{read}(R, i) \}
\]
\end{definition}

\begin{definition}
\[
\textsf{write\_parameters}(R) = \{ i \mid \textsf{write}(R, i) \}
\]
\end{definition}

\vspace{0.5cm}
\noindent
Now let us consider an mCRL2 summand $P$ of the following shape:
\[
P(d) = c(d) \rightarrow a(f(d)) . P(g(d))
\]

\noindent
For such a summand \cite{DBLP:conf/hvc/MeijerKBP14} defines the following approximations of read and write parameters:
\[
\textsf{read}_s(P, i) =
\begin{array}{ll}
d_i \in \textsf{freevars}(c(d)) & \lor \\
(d_i \in \textsf{freevars}(g_i(d)) \land d_i \neq g_i(d)) & \lor \\   
\exists 1 \leq k \leq m: (d_i \in \textsf{freevars}(g_k(d)) \land i \neq k)   
\end{array}
\]

\[
\textsf{write}_s(P, i) = (d_i \neq g_i)
\]

\vspace{0.5cm}
\noindent
Indeed we have $\textsf{read}_s(P, i) \Rightarrow \textsf{read}(P, i)$ and
$\textsf{write}_s(P, i) \Rightarrow \textsf{write}(P, i)$.

\newpage
\section{List Decision Diagrams}

A List Decision Diagram (LDD) is a DAG. It has two types of leaf nodes, \textsf{false} and \textsf{true}, or 0 and 1. The third type of node has a label $a$ and two successors \var{down} and \var{right}, or $=$ and $>$.
An LDD represents a set of lists, as follows:

\begin{equation*}
\begin{array}{lll}
  \llbracket \textsf{false} \rrbracket & = & \emptyset \\
  \llbracket \textsf{true} \rrbracket & = & \{ \emptylist \} \\
  \llbracket \textsf{node}(v, \var{down}, \var{right}) \rrbracket & = & 
    \{ vx \mid x \in \llbracket \var{down} \rrbracket \} \cup \llbracket \var{right} \rrbracket
\end{array}
\end{equation*}

\noindent
In \cite{MeijerBlomPol2008} an LDD is defined as

\begin{definition}
A List decision diagram (LDD) is a
directed acyclic graph with the following properties:
\begin{enumerate}
    \item There is a single root node and two terminal nodes 0 and 1.
    \item Each non-terminal node $p$ is labeled with a value $v$, denoted by $val(p) = v$,
and has two outgoing edges labeled $=$ and $>$ that point to nodes denoted by
$p[x_i = v]$ and $p[x_i > v]$.
    \item For all non-terminal nodes $p$, $p[x_i = v] \neq 0$ and $p[x_i > v] \neq 1$.
    \item For all non-terminal nodes $p$, $val(p[x_i > v]) > v$.
    \item There are no duplicate nodes.
 \end{enumerate}
\end{definition}

\noindent
LDDs are well suited to store lists that differ in only a few positions.
Consider the transition relation $R$ on $S = \mathbb{N}^{10}$ with initial state
$x = [0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0]$, that is defined by

\[
\textsf{if } x_5 > 0 \textsf{ then begin } x_5 := x_5 - 1; x_6 := x_6 + 1 \textsf{ end}
\]

Clearly this is a sparse relation with $\textsf{used}(R) = \{ 5, 6 \}$. The state space
consists of 11 states that differ only in the 5th and 6th parameter. It can be compactly
represented using an LDD, see the figure below. For our applications we use the LDD implementation that is part of the Sylvan multi-core framework for decision diagrams, see
\cite{DBLP:journals/sttt/DijkP17}.

\includegraphics[width=15cm]{ldd_if_then.pdf}

\newpage
\section{Reachability}

\subsection{Computing the set of reachable states}
A straightforward algorithm to compute the set of reachable states from an initial state
$x \in S$ is the following.

\begin{algorithm}[ht]
\caption{Reachability}
\label{alg:reachability}
\begin{algorithmic}[1]
\Function{ReachableStates}{$R$, $x$}
\State $\var{visited} := \{ x \}$
\State $\var{todo} := \{ x \}$
\While {$\var{todo} \neq \emptyset$}
    \State $\var{todo} := \textsf{next}(R, \var{todo}) \setminus \var{visited}$
    \State $\var{visited} := \var{visited} \cup \var{todo}$
\EndWhile
\State \Return \var{visited}
\EndFunction
\end{algorithmic}
\end{algorithm}

\noindent
There are two bottlenecks in this algorithm. First of all the set \var{visited} may be large and therefore consume a lot of memory. Second, the computation of \textsf{next}(R, \var{todo}) may become 
expensive once \var{todo} becomes large.

\subsection{Reachability with learning}
To reduce the memory usage, we store the sets $visited$ and $todo$ using LDDs. It is not always feasible to store the entire transition relation $R$ using LDDs, because it may be huge or even have infinite size. To deal with this, we only store the subset $L$ of $R$ that is necessary for the reachability computation in an LDD. The relation $L$ is computed (learned) on the fly.

\begin{algorithm}[ht]
\caption{Reachability with learning}
\label{alg:reachability}
\begin{algorithmic}[1]
\Function{ReachableStates}{$R$, $x$}
\State $\var{visited} := \{ x \}$
\State $\var{todo} := \{ x \}$
\State $L := \emptyset$ \Comment{$L$ is the learned relation}
\While {$\var{todo} \neq \emptyset$}
    \State $L := L \cup \{ (x,y) \in R \mid x \in \var{todo} \} $ \Comment{This operation is expensive}
    \State $\var{todo} := \textsf{next}(L, \var{todo}) \setminus \var{visited}$ \Comment{This operation is cheap}
    \State $\var{visited} := \var{visited} \cup \var{todo}$
\EndWhile
\State \Return \var{visited}
\EndFunction
\end{algorithmic}
\end{algorithm}

\newpage
\subsection{Reachability of a sparse relation}

Suppose that we have a sparse relation $R$, i.e. the number of read and write parameters is small. In that case we can use projections to increase the efficiency. \\

\begin{definition}
The projection of a state $x \in S$ with respect to a set of parameter indices $\{ i_1, \ldots i_k \}$ with $1 \leq i_1 < \ldots < i_k \leq m$ is defined as
\[
\textsf{project}(x, \{ i_1, \ldots, i_k \} ) = (x_{i_1}, \ldots, x_{i_k})
\]

\noindent
We lift this to a relation $R$ with read parameter indices $I_r$ and write parameter indices $I_w$ as follows:

\[
\textsf{project}(R, I_r, I_w) = \{ (\textsf{project}(x,I_r), \textsf{project}(y,I_w)) \mid (x,y) \in R \}
\]
\end{definition}

\noindent
The application of a projected relation to an unprojected state is defined using the function \textsf{relprod}. The Sylvan function \textsf{relprod} implements this, or something similar.

\begin{definition}
Let $R$ be a relation with read parameter indices $I_r$ and write parameter indices $I_w$, let $x \in S$ and let
 $\hat{R} = \textsf{project}(R, I_r, I_w)$. Then we define

\[
    \textsf{relprod}(\hat{R}, x, I_r, I_w) =
        \{ 
          x[I_w = \hat{y}] \mid \textsf{project}(x, I_r) = \hat{x} \land 
          (\hat{x}, \hat{y}) \in \hat{R}
        \}
\]

\[
    \textsf{relprev}(\hat{R}, y, I_r, I_w, X) =
        \{ 
           x \in X \mid y \in \textsf{relprod}(\hat{R}, x, I_r, I_w)
        %   y[I_r = \hat{x}] \mid \textsf{project}(y, I_w) = \hat{y} \land 
        %   (\hat{x}, \hat{y}) \in \hat{R}
        \}
\]
\end{definition}

\begin{algorithm}[h]
\caption{Reachability of a sparse relation using projections}
\label{alg:reachability3}
\begin{algorithmic}[1]
\Function{ReachableStates}{$R$, $x$}
\State $\var{visited} := \{ x \}$
\State $\var{todo} := \{ x \}$
\State $L := \emptyset$ \Comment{$L$ is a projected relation}
\State $I_r, I_w := \textsf{read\_parameters}(R), \textsf{write\_parameters}(R)$
\While {$\var{todo} \neq \emptyset$}
    \State $L := L \cup \textsf{project}(\{ (x,y) \in R \mid x \in \var{todo} \}, I_r, I_w) $  \label{line:r_successors}
    \State $\var{todo} := \textsf{relprod}(L, \var{todo}, I_r, I_w) \setminus \var{visited}$ 
    \State $\var{visited} := \var{visited} \cup \var{todo}$
\EndWhile
\State \Return \var{visited}
\EndFunction
\end{algorithmic}
\end{algorithm}

\noindent
In this new version of the algorithm, the computation of the successors in line \ref{line:r_successors} is still the bottleneck:

\[
L := L \cup \textsf{project}(\{ (x,y) \in R \mid x \in \var{todo} \}, I_r, I_w)
\]

\noindent
An important observation is that the same result can be obtained by applying the projected relation to the projected arguments:

\[
L := L \cup \{ (x,y) \in \textsf{project}(R, I_r, I_w) \mid x \in \textsf{project}(\var{todo}, I_r) \}
\]

\noindent
For our applications, the set $\textsf{project}(\var{todo}, I_r \cup I_w) \}$ is typically much smaller than $\var{todo}$, which means that a lot of duplicate successor computations in line \ref{line:r_successors} are avoided.

\newpage
\subsection{Reachability of a union of sparse relations}

We now consider a relation $R$ that is the union of a number of sparse relations. Examples of these sparse relations are the summands of an LPS or of a PBES in SRF format.

\[
R = \bigcup_{i=1}^{n} R_i
\]

\begin{algorithm}[h]
\caption{Reachability of a union of sparse relations}
\label{alg:reachability4}
\begin{algorithmic}[1]
\Function{ReachableStates}{$\{R_1, \ldots, R_n\}$, $x$}
\State $\var{visited} := \{ x \}$
\State $\var{todo} := \{ x \}$
\For {$1 \leq i \leq n$}
    \State $L_i := \emptyset$
    \State $I_{r,i}, I_{w,i} := \textsf{read\_parameters}(R_i), \textsf{write\_parameters}(R_i)$
\EndFor
\While {$\var{todo} \neq \emptyset$}
    \For {$1 \leq i \leq n$}
        \State $L_i := L_i \cup \{ (x,y) \in \textsf{project}(R_i, I_{r,i}, I_{w,i}) \mid x \in \textsf{project}(\var{todo}, I_{r,i})  \setminus \textsf{domain}(L_i) \}$
    \EndFor
    \State $\var{todo} := \left( \bigcup\limits_{i=1}^n \textsf{relprod}(L_i, \var{todo}, I_r, I_w) \right) \setminus \var{visited}$
    \State $\var{visited} := \var{visited} \cup \var{todo}$
\EndWhile
\State \Return \var{visited}
\EndFunction
\end{algorithmic}
\end{algorithm}
\noindent
Note that in line 9 another optimization has been applied, by excluding elements in the projected todo list that are already in the domain of $L_i$.
For all values in $\textsf{domain}(L_i)$ the outgoing transitions have already been determined.
We can also replace $\textsf{domain}(L_i)$ by a set $X$ that keeps track of all values of $x$ that have already been processed.
Let $X$ be initially the empty set and updated to $X \gets X \cup \{x \in \textsf{project}(\var{todo}, I_{r,i})\}$ at line 9.
The set $X$ contains all elements in $\textsf{domain}(L_i)$ but also all $x$ with no outgoing transitions.
In practice, the transitions of $R_i$ are computed on-the-fly and this additional caching can avoid these computations with the downside that it requires more memory.

\newpage
\subsection{Reachability with chaining}

For symbolic reachability it can be useful to reduce the amount of breadth-first search iterations because this also reduces the amount of symbolic operations that have to be applied.
Updating the todo set after applying each sparse relation can potentially increase the amount of states that are visited and thus reduce the amount of breadth-first search iterations necessary.
Note that we only add the states for which all sparse relations have been applied to visited.

\begin{algorithm}[H]
\caption{Reachability of a union of sparse relations}
\label{alg:reachability_with_chaining}
\begin{algorithmic}[1]
\Function{ReachableStates}{$\{R_1, \ldots, R_n\}$, $x$}
\State $\var{visited} := \{ x \}$
\State $\var{todo} := \{ x \}$
\For {$1 \leq i \leq n$}
    \State $L_i := \emptyset$
    \State $I_{r,i}, I_{w,i} := \textsf{read\_parameters}(R_i), \textsf{write\_parameters}(R_i)$
\EndFor
\While {$\var{todo} \neq \emptyset$}
    \State $\var{todo1} := \var{todo}$
    \For {$1 \leq i \leq n$}
        \State $L_i := L_i \cup \{ (x,y) \in \textsf{project}(R_i, I_{r,i}, I_{w,i}) \mid x \in \textsf{project}(\var{todo1}, I_{r,i})  \setminus \textsf{domain}(L_i) \}$
        \State $\var{todo1} := \var{todo1} \cup \textsf{relprod}(L_i, \var{todo1}, I_r, I_w)$
    \EndFor
    \State $\var{visited} := \var{visited} \cup \var{todo}$
    \State $\var{todo} := \var{todo1} \setminus \var{visited}$
\EndWhile
\State \Return \var{visited}
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Reachability with deadlock detection}

To detect \emph{deadlocks}, \emph{i.e.}, states with no outgoing transitions, during reachability we have to determine which states in the todo sets have no outgoing transitions after applying all the transitions groups.
This can be achieved as follows by considering the predecessors.

\begin{algorithm}[h]
\caption{Reachability of a union of sparse relations}
\label{alg:reachability_with_deadlock}
\begin{algorithmic}[1]
\Function{ReachableStates}{$\{R_1, \ldots, R_n\}$, $x$}
\State $\var{visited} := \{ x \}$
\State $\var{todo} := \{ x \}$
\State $\var{deadlocks} := \emptyset$
\For {$1 \leq i \leq n$}
    \State $L_i := \emptyset$
    \State $I_{r,i}, I_{w,i} := \textsf{read\_parameters}(R_i), \textsf{write\_parameters}(R_i)$
\EndFor
\While {$\var{todo} \neq \emptyset$}
    \State $\var{potential\_deadlocks} := \var{todo}$
    \For {$1 \leq i \leq n$}
        \State $L_i := L_i \cup \{ (x,y) \in \textsf{project}(R_i, I_{r,i}, I_{w,i}) \mid x \in \textsf{project}(\var{todo}, I_{r,i})  \setminus \textsf{domain}(L_i) \}$
    \EndFor
    \State $\var{todo} := \left( \bigcup\limits_{i=1}^n \textsf{relprod}(L_i, \var{todo}, I_r, I_w) \right) \setminus \var{visited}$
    \State $\var{potential\_deadlocks} := \var{potential\_deadlocks} \setminus \left( \bigcup\limits_{i=1}^n \textsf{relprev}(L_i, \var{todo}, I_r, I_w, \var{potential\_deadlocks}) \right)$
    \State $\var{visited} := \var{visited} \cup \var{todo}$
    \State $\var{deadlocks} := \var{deadlocks} \cup \var{potential\_deadlocks}$
\EndWhile
\State \Return \var{visited}
\EndFunction
\end{algorithmic}
\end{algorithm}
For the chaining strategy we remove predecessors from the potential deadlocks at the end of every transition group iteration (on line 11 of Algorithm~\ref{alg:reachability_with_chaining}) using $\var{todo1}$ instead of $\var{todo}$.

\subsection{Joining relations}
If two or more of the relations $R_i$ have approximately the same set of read and write parameters, it can be beneficial to join them into one relation. In \cite{Meijer2019} this is called combining transition groups. 
In order to
determine how well two relations match, we define a bit pattern for a relation that contains the read and write information of the parameters.

\begin{definition}
The read write pattern of a relation $R$ is defined as
\[
\textsf{read\_write\_pattern}(R) = [r_1, w_1, \ldots, r_m, w_m]
\]
with
\[
  r_i = \textsf{read}(R, i) \text{ and } w_i = \textsf{write}(R, i) \hspace{1cm} (1 \leq i \leq m)
\]
\end{definition}
For two read write patterns $p$ and $q$, we define $p \lor q$ as the bitwise or of both patterns. In other words, if $r = p \lor q$, then $r_i = p_i \lor q_i ~ (1 \leq i \leq 2m)$. Similarly we say that $p \leq q$ iff $p_i \leq q_i$ for $1 \leq i \leq 2m$.

\begin{example}
Let $S = \mathbb{N} \times \mathbb{N}$ and let $T$ and $U$ be relations on the variables $x$, $y$. Let $T$ be defined by 
$(x,y) \rightarrow (x + 1, x)$
and let $U$ be defined by
$x, y := x + 2, y$.
In this case $x$ is a read independent parameter in both $T$ and $U$, but according to the definition $x$ is not a read independent parameter of 
$T \cup U$.
Hence $\textsf{read\_write\_pattern}(T) = 1101$,
$\textsf{read\_write\_pattern}(U) = 1100$, and
$\textsf{read\_write\_pattern}(T \cup U) = 1111$. 
\end{example}

\subsubsection{Row subsumption}
In \cite{Meijer2019} a notion called \emph{row subsumption} is
introduced for joining relations. This notion is based on an extension to the theory, which ensures that the following property holds for two relations $T$ and $U$:

\begin{equation} \label{bitwise_or}
\textsf{read\_write\_pattern}(T \cup U) = \textsf{read\_write\_pattern}(T) \lor \textsf{read\_write\_pattern}(U)
\end{equation}

\noindent
It works as follows. Suppose we have a transition $(x, y) \in T$, and let $L$ be the projected transition relation corresponding to $T \cup U$. Then we insert the special value $\blacktriangle$ in $L$ for all entries of $y$ that correspond to a copy parameter of $T$ (i.e. with read write values 00). The meaning of this special value is that the corresponding parameter will not be overwritten by $L$. The 

This is achieved by using
$\textsf{relprod}^\blacktriangle$ instead of \textsf{relprod}, which is
defined as follows:

\[
x[I = y]^\blacktriangle = z \text{ with } z_r = 
  \left\{
    \begin{array}{ll}
         x_r & \text{if } r \notin I \text{ or } y_r = \blacktriangle \\
         y_r & \text{otherwise}
    \end{array}
  \right. 
\]

\[
    \textsf{relprod}^\blacktriangle(\hat{R}, x, I_r, I_w, X) =
        \{ 
          x[I_w = \hat{y}]^\blacktriangle \mid \textsf{project}(x, I_r) = \hat{x} \land 
          (\hat{x}, \hat{y}) \in \hat{R}
        \}
\]

N.B. In the Sylvan \textsf{relprod} function this functionality is implemented in a slightly different way, using a concept called 'copy nodes'. It requires that the matrix $L$ is assembled using the function \textsf{union\_cube\_copy} instead of \textsf{union\_cube}.  

\subsection{An algorithm for partitioning a union of relations}
In this section we sketch an approach for making a partition of a union of relations $R = \bigcup_{i=1}^n R_i$ that is based on property \ref{bitwise_or}. The goal is to create groups of relations $R_i$ that have approximately the same read write patterns. The main idea is that each group of the partition can be characterized by a read write pattern.

\vspace{0.5cm}
\noindent
If we choose $K$ read write patterns $p_1, \ldots, p_K$ such that
\[
\bigvee\limits_{k=1}^K p_k = \textsf{read\_write\_pattern}(R)
\]
and
\[
\forall 1 \leq i \leq n: \exists 1 \leq k \leq K: \textsf{read\_write\_pattern}(R_i) \leq p_k
\]
then we can define $K$ groups by assigning relation $R_i$ to group $k$ if $\textsf{read\_write\_pattern}(R_i) \leq p_k$. Note that a
relation $R_i$ can match multiple read write patterns $p_k$, but in that case an arbitrary pattern may be chosen.

\vspace{0.5cm}
\noindent
A heuristic for selecting suitable patterns $p_k$ is to choose them such that both
\[
\sum\limits_{1 \leq k \leq K} \textsf{bitcount}(p_k)
\]
and
\[
\sum\limits_{1 \leq k < l \leq K} \textsf{bitcount}(p_k \land p_l)
\]
are small. For example an SMT solver might be used to determine them.

\newpage
\section{Application: LPS Reachability}
Consider the following untimed linear process specification $P$, with initial state $d_0$.
\[
\begin{array}{l}
P(d: D)=
\sum\limits_{i\in I}\sum\limits_{e_i}c_i(d, e_i)\rightarrow a_i(f_i(d,e_i)) \cdot P(g_i(d,e_i))
\end{array}
\]

\noindent
Each summand $i \in I$ defines a transition relation $R_i$ characterized by

\[
\left\{
\begin{array}{ll}
    e_i & \text{a sequence of summation variables} \\
    c_i(d, e_i) & \text{a condition} \\
    a_i(f_i(d,e_i)) & \text{a transition label} \\
    g_i(d,e_i) & \text{the successor states} \\
\end{array}
\right.
\]

\vspace{0.5cm}
\noindent
The set of states is $D$, which is naturally defined as a
Cartesian product

\[
D = D_1 \times \ldots \times D_m
\]

In order to apply the reachability algorithm, we need the following ingredients:

\begin{center}
\label{table:ingredients}
\begin{tabular}{ |l|l| }
\hline
$\{ (x,y) \in R_i \mid x \in todo \}$ & enumerate solutions of condition \\
conversion between states and LDDs & map values to integers \\
$\textsf{read\_parameters}(R_i)$ & syntactic analysis \\
$\textsf{write\_parameters}(R_i)$ & syntactic analysis \\
$\textsf{project}(R_i, I_{r,i}, I_{w,i})$ & discard unused parameters \\
$\textsf{project}(\var{todo}, I_{r,i} \cup I_{w,i})$ & \textsf{project} (Sylvan function) \\
$\textsf{relprod}(L_i, \var{todo}, I_r, I_w)$ & \textsf{relprod} (Sylvan function) \\
\hline
\end{tabular}
\end{center}

\noindent
For a summand of the shape $R_i = c_i \rightarrow a_i(f_i) \cdot P(g_i)$ we will now define the read and write dependencies.

\[
\textsf{read}(R_i, j) = 
d_j \in \textsf{freevars}(c_i) \lor
\exists 1 \leq k \leq m: (d_j \in \textsf{freevars}(g_{i,k}) \land (d_k \neq d_j \lor g_{i,j} \neq d_j))
\]

\[
\textsf{write}(R_i, j) = (d_j \neq g_{i,j})
\]

\newpage
\section{Application: PBES Reachability}

\begin{definition}
A parameterised Boolean equation system (PBES) is a sequence of equations as defined by the following grammar:
\[
\mathcal{E} ::= \emptyset \mid (\mu X(d:D) = \varphi) \mathcal{E} \mid (\mu X(d:D) = \varphi) \mathcal{E}
\]
\end{definition}
where $\emptyset$ is the empty PBES, $\mu$ and $\nu$ denote the least and greatest fixpoint operator, respectively, and $X \in \chi$ is a predicate
variable of sort $D \rightarrow B$. The right-hand side $\varphi$ is a syntactically monotone predicate formula. Lastly, $d \in V$ is a parameter of
sort $D$.

\begin{definition}
Let $\mathcal{E}$ be a PBES. Then $\mathcal{E}$ is in standard recursive form (SRF) iff for all $\sigma_i X_i(d:D) = \varphi) \in \mathcal{E}$, where
$\varphi$ is either disjunctive or conjunctive, i.e., the equation for $X_i$ has the shape
\[
  \sigma_i X_i(d:D) = \bigvee\limits_{j \in J_i} \exists e_j: E_j . f_{ij}(d,e_j) \land X_{g_{ij}}(h_{ij}(d, e_j))
\]
or
\[
  \sigma_i X_i(d:D) = \bigwedge\limits_{j \in J_i} \forall e_j: E_j . f_{ij}(d,e_j) \implies X_{g_{ij}}(h_{ij}(d, e_j)),
\]
where $d = (d_1, \ldots, d_m)$.
\end{definition}

\subsection{PBES Reachability}
We will now define reachability for a PBES.
The set of states is $S = \{ X_i(d) \mid 1 \leq i \leq n \land d \in D \}$.
Each summand $(i,j)$ with $j \in J_i$ defines a transition relation $R_{ij}$ that is characterized by the parameters

\[
\left\{
\begin{array}{ll}
    \sigma_i & \text{a fixpoint symbol} \\
    e_j & \text{a sequence of quantifier variables} \\
    (X = X_i) \land f_{ij}(d,e_j) & \text{a condition} \\
    X_{g_{ij}}(h_{ij}(d, e_j)) & \text{the successor states,} \\
\end{array}
\right.
\]
where $X(d)$ is the current state.

In order to apply the reachability algorithm, we need the following ingredients (exactly the same as for LPSs):

\begin{center}
\label{table:pbes_ingredients}
\begin{tabular}{ |l|l| }
\hline
$\{ (x,y) \in R_i \mid x \in todo \}$ & enumerate solutions of condition \\
conversion between states and LDDs & map values to integers \\
$\textsf{read\_parameters}(R_i)$ & syntactic analysis \\
$\textsf{write\_parameters}(R_i)$ & syntactic analysis \\
$\textsf{project}(R_i, I_{r,i}, I_{w,i})$ & discard unused parameters \\
$\textsf{project}(\var{todo}, I_{r,i} \cup I_{w,i})$ & \textsf{project} (Sylvan function) \\
$\textsf{relprod}(L_i, \var{todo}, I_r, I_w)$ & \textsf{relprod} (Sylvan function) \\
\hline
\end{tabular}
\end{center}

\subsection{Splitting Conditions}

Symbolic reachability is most effective when the number of read/write parameters and the number of transitions per transition group are small.
We can split the disjunctive conditions in a conjunctive equation as follows:
Consider a PBES $\mathcal{E}$ in SRF with an equation $\sigma_i X_i(d:D) \in \mathcal{E}$ of the shape:
\begin{align*}
  \sigma_i X_i(d:D) &= \bigwedge\limits_{j \in J_i} \forall e_j : E_j . (\bigvee\limits_{k \in K_{ij}} f_{ijk}(d,e_j)) \implies X_{g_{ij}}(h_{ij}(d, e_j)) \\  
  \sigma_i X_i(d:D) &= \bigwedge\limits_{j \in J_i} \forall e_j : E_j . \bigwedge\limits_{k \in K_{ij}} f_{ijk}(d,e_j) \implies X_{g_{ij}}(h_{ij}(d, e_j)) \\
  \sigma_i X_i(d:D) &= \bigwedge\limits_{j \in J_i}\bigwedge\limits_{k \in K_{ij}} \forall e_j : E_{ij} .  f_{ijk}(d,e_j) \implies X_{g_{ij}}(h_{ij}(d, e_j))
\end{align*}
This transformation is safe and allows us to consider every $f_{ijk}(d,e_j))$ as its own transition group.
A similar transformation can be applied to conjunctions in a disjunctive equation.

We can apply another transformation to conjunctive conditions inside conjunctive equations.
By introducing a new equation for every conjunctive clause $f_{ijk}(d,e_j))$, for $k \in K_j$, we can remain in SRF by the following transformation:
\begin{align*}
  \sigma_i X_i(d:D) &= \bigwedge\limits_{j \in J_i} \forall e_j : E_j . (\bigwedge\limits_{k \in K_{ij}} f_{ijk}(d,e_j)) \implies X_{g_{ij}}(h_{ij}(d, e_j)) \\  
  \sigma_i X_i(d:D) &= \bigwedge\limits_{j \in J_i} \forall e_j : E_j . \bigvee\limits_{k \in K_{ij}} (\neg f_{ijk}(d,e_j) \lor X_{g_{ij}}(h_{ij}(d, e_j))) \\
  \sigma_i X_i(d:D) &= \bigwedge\limits_{j \in J_i} \forall e_j : E_j .  \textit{true} \implies Y_j(h_{ij}(d, e_j), e_j)
\end{align*}
Where $Y_j$ is a fresh name and its equation defined as follows:
\begin{align*}
  \nu Y_j(d : D, e_j : E_j) &= (\bigvee\limits_{k \in K_{ij}} \neg f_{ijk}(d,e_j) \land Y_j(d, e_j)) \lor (\textit{true} \land X_{g_{ij}}(d, e_j))
\end{align*}
Note that this transformations makes the variable $e_j$ visible in the state space.
Furthermore, this parameter is also quantified without a condition.
In practice, this means that the latter transformation is probably unwanted in the presence of quantifiers.
Note that every $j \in J_i$ can be transformed according to whether the condition is conjunctive or disjunctive respectively, or not transformed at all.
The cases for a disjunctive or conjunctive condition within a disjunctive equation are completely dual.

We can also observe that in the equation for $Y_j$ there is always a dependency on $X_{g_{ij}}(h_{ij}(d, e_j))$, whereas, in the original equation this was guarded by $(\bigwedge\limits_{k \in K_j} f_{ijk}(d,e_j))$.
Alternatively, we can also consider the following equation for $Y_j$ such that $X_{g_{ij}}(h_{ij}(d, e_j))$ still has (weaker) guards.

\begin{align*}
  \nu Y_j(d : D, e_j : E_j) &= \bigvee\limits_{k \in K_j} \textit{true} \land Y_{jk}(d, e_j) \\
  \nu Y_{jk}(d : D, e_j : E_j) &= f_{ijk}(d,e_j) \implies X_{g_{ij}}(h_{ij}(d, e_j))) & \text{for } k \in K_i
\end{align*}


\newcommand{\pre}[3][]{\ensuremath{\textsf{pre}(#2,#3)}}
\newcommand{\cpre}[4][]{\ensuremath{\textsf{cpre}_{#2}(#3,#4)}}
\newcommand{\spre}[4][]{\ensuremath{\textsf{spre}_{#2}(#3,#4)}}

\newpage
\section{Parity Game Solving}

Let $G = (V, E, r, (V_0, V_1))$ be a parity game where $V_0$ and $V_1$ are disjoint sets of vertices in $V$ owned by $0$ (even) and $1$ (odd) respectively.
Furthermore, $E \subseteq V \times V$ is the edge relation, and we denote elements of it by $v \rightarrow u$ iff $(v, u) \in E$.
Finally, $r(v)$ is the priority function assigning a priority to every vertex $v \in V$.
For a non-empty set $U \subseteq V$ and a player $\alpha$, the control predecessor of set $U$ contains the vertices for which player $\alpha$ can force entering $U$ in one step.
Let $\pre{U}{V} = \{ u \in U \mid \exists v \in V: u \rightarrow v \}$ then it is defined as follows:
\begin{equation*}
  \cpre{\alpha}{G}{U} = (V_{\alpha} \cap \pre{G}{U}) \cup (V_{1-\alpha} \setminus \pre{G}{V \setminus U})
\end{equation*}

The $\alpha$-attractor into $U$, denoted $\attr{\alpha}{U,V}$, is the set of vertices for which player $\alpha$ can force any play into $U$. 
We define $\attr{\alpha}{U,V}$ as $\bigcup\limits_{i \ge 0} \attr[i]{\alpha}{U,V}$, the limit of approximations\label{def:attractor} of the sets $\attr[n]{\alpha}{U,V}$, which are inductively defined as follows:
\[
\begin{array}{lcl}
\attr[0]{\alpha}{G, U} & = & U \\
\attr[n+1]{\alpha}{G, U} & = & \attr[n]{\alpha}{G, U} \\
      & \cup & \cpre{\alpha}{G}{\attr[n]{\alpha}{G, U}} \\
\end{array}
\]
We reformulate this into
\[
\begin{array}{lcl}
\attr[0]{\alpha}{G, U} & = & U \\
\attr[n+1]{\alpha}{G, U} & = & \attr[n]{\alpha}{G, U} \\
      & \cup & (V_{\alpha} \cap \pre{V}{\attr[n]{\alpha}{G, U})} \\
      & \cup & (V_{1 - \alpha} \cap (V \setminus \pre{V}{V \setminus \attr[n]{\alpha}{G, U})} \\
\end{array}
\]

This can be further optimised by avoiding two intersections and only computing predecessors within $V_{1 - \alpha}$. 
Furthermore, for a union of sparse relations $\rightarrow \,= \bigcup_{i=1}^{n} \rightarrow_i$ we can also apply the transition relations $\rightarrow_i$ directly instead of determining $\rightarrow$ first.
For $\pred_i(U, V) = \{ u \in U \mid \exists v \in V: u \rightarrow_i v \}$ we define:

\[
\begin{array}{lcl}
\attr[0]{\alpha}{G, U} & = & U \\
\attr[n+1]{\alpha}{G, U} & = & \attr[n]{\alpha}{G, U} \\
      & \cup & \bigcup_{i=1}^{n} \textsf{pre}_i(V_{\alpha}, \attr[n]{\alpha}{G, U}) \\
      & \cup & (V_{1 - \alpha} \setminus \bigcup_{i=1}^{n} \textsf{pre}_i(V_{1 - \alpha}, V \setminus \attr[n]{\alpha}{G, U}) \\
\end{array}
\]

First of all, we implement a variant of $\cpre{\alpha}{G}{U}$ that uses the union of sparse relations. The parameter $Z_\var{outside}$ can be used as an optimisation to reduce the amount of states considered, but can always be equal to $V$. In the attractor set computation this will be equal to $V \setminus \attr[n]{\alpha}{G, U})$.

\begin{algorithm}[H]
\caption{Control predecessor set computation for a union of sparse relations}
\label{alg:predecessor}
\begin{algorithmic}[1]
\Function{CPre$_\alpha$}{$G = (V, E, r, (V_0, V_1)$, $\var{U}$, $\var{Z}_\var{outside}$}
  \State $\var{P} := \pre{G}{U}$
  \State $\var{P}_\alpha := P \cap V_\alpha$
  \State $\var{P}_\var{forced} := P \cap V_{1 - \alpha}$
  
  \For {$1 \leq i \leq n$}
    \State $\var{P}_\var{forced} := \var{P}_\var{forced} \setminus \textsf{pre}_i(\var{P}_\textit{forced}, \var{Z}_\var{outside})$  
  \EndFor
  
  \State \Return $\var{P}_\alpha \cup \var{P}_\var{forced}$
\EndFunction
\end{algorithmic}
\end{algorithm}

For the actual implementation of the attractor set scheme we make two additional observations.
Instead of determining all predecessors for states in $V \setminus \attr[n]{\alpha}{U,V})$ in the final step we only have to determine predecessors that can actually reach $V \setminus \attr[n]{\alpha}{U,V})$ in one step.
Furthermore, we can actually keep track of the states that have been added in the last iteration and were not yet part of the attractor set.
We then only have to compute predecessors with respect to this todo set.

\begin{algorithm}[H]
\caption{Attractor set computation for a union of sparse relations}
\label{alg:attractor_set}
\begin{algorithmic}[1]
\Function{Attr$_\alpha$}{$G = (V, E, r, (V_0, V_1)$, $\var{U}$}
\State $\var{todo} := U$
\State $\var{Z} := U$
\State $\var{Z}_\var{outside} := V \setminus X$

\While {$\var{todo} \neq \emptyset$}
  \State $\var{todo} := \textsf{CPre}_\alpha(G, \var{Z}, \var{Z}_\var{outside})$
  \State $\var{Z} := \var{Z} \cup \var{todo}$
  \State $\var{Z}_\var{outside} := \var{Z}_\var{outside} \setminus \var{todo}$
\EndWhile
\State \Return \var{Z}
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Zielonka}

The standard Zielonka solving algorithm, defined in Algorithm~\ref{alg:zielonka} requires that every vertex has an outgoing edge.
If this is the case then the graph is called \emph{total}.
We can achieve this by extending every disjunctive PBES equation with $\textsf{true} \land X_\textsf{false}$ where $X_\textsf{false}$ is defined as $\mu X_\textsf{false} = \textsf{true} \land X_\textsf{false}$ and similarly for the conjunctive PBES equations with $X_\textsf{true}$.
However, adding these unnecessary transitions can be costly.

Therefore, if we perform the deadlock detection we can avoid extending the PBES and obtain a total graph by performing the preprocessing step described in Algorithm~\ref{alg:preprocess}.
Every disjunctive vertex that is a deadlock is won by player odd (previously indicated by a transition to $X_\textsf{false}$) and every conjunctive vertex that is a deadlock by player even.
If we compute the attractors to these won vertices then the resulting graph is total and can be used in the Zielonka algorithm as follows $\textsc{zielonka}(\textsc{preprocess}(V, D, \emptyset, \emptyset))$.

\begin{algorithm}[H]
\caption{Preprocess the graph to be total and remove deadlocks $D$ already solved vertices $W_0, W_1$ where $W_0$ is won by even and $W_1$ by odd.}
\label{alg:preprocess}
\begin{algorithmic}[1]
\Function{Preprocess}{$V, D, W_0, W_1$}
	\State {$W'_0 \gets W_0 \cup (D \cap V_1)$}	
	\State {$W'_1 \gets W_1 \cup (D \cap V_0)$}
	
	\State {$W'_0 \gets \attr{0}{W'_0, V}$}
	\State {$W'_1 \gets \attr{1}{W'_1, V}$}
	
	\State \Return $V \setminus (W'_0 \cup W'_1)$
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Zielonka}
\label{alg:zielonka}
\begin{algorithmic}[1]
\Function{Zielonka}{$V$}
\If {$V = \emptyset$}
  \State \Return {$\emptyset, \emptyset$}
\EndIf
\State $m:=\min (\{r(v)\mid v\in V\})$ 
\State $\alpha := m \bmod 2$
\State $U:=\{v\in V\mid r(v)=m\}$ 
\State $A := \attr{\alpha}{U,V}$
\State $W_{0}^{\prime },W_{1}^{\prime}:= \textsc{Zielonka}(V\setminus A)$
\If {$W_{1-\alpha }^{\prime }= \emptyset$}
  \State $W_{\alpha },W_{1-\alpha }:=A\cup W_{\alpha }^{\prime},\emptyset$ 
\Else
  \State $B := \attr{1-\alpha}{W_{1-\alpha }^{\prime }, V}$
  \State $W_{0},W_{1}:=\textsc{Zielonka}(V\setminus B)$ 
  \State $W_{1-\alpha } := W_{1-\alpha }\cup B$ 
\EndIf
\State \Return $W_{0},W_{1}$
\EndFunction
\end{algorithmic}
\end{algorithm}

\newpage
\subsection{Partial Solving and Safe (Chaining) Attractors}

We recall some of the definitions necessary for the purpose for partial solving (incomplete) parity games that is introduced in~\cite{DBLP:conf/tacas/LaveauxWW22}, and provide pseudocode for these algorithms as well.
First of all, we introduce the notion of an incomplete parity game $\Game = (G, I)$, where $I \subseteq V$ is a set of incomplete vertices.

\begin{equation*}
  \spre{\alpha}{G}{U} = (V_{\alpha} \cap \pre{G}{U}) \cup (V_{1-\alpha} \setminus (\pre{G}{V \setminus U} \cup I))
\end{equation*}

Secondly, we extend our attractor set computation to allow chaining through the predecessors.
For this purpose we introduce a chaining predecessor function $\textsf{chained\_pre}(G, U, W)$ that has a parameter $W$ which defines the set of vertices where chaining is allowed.
A function is a chaining predecessor iff it returns a set $P = \textsf{chained\_pre}(G, U, W)$ such that $\pre{G}{U} \subseteq P$ and $P \subseteq \{ u \in U \mid \exists v \in V: u \rightarrow_W^* v \}$ such that $u \rightarrow_W^* v$ iff there is a sequence of vertices $s \rightarrow u_0 \rightarrow u_1 \rightarrow \ldots \rightarrow _v $ for which $u_0, u_1, \ldots \in W$.
 
 \begin{algorithm}[H]
 \caption{Safe control predecessor set computation for a union of sparse relations}
 \label{alg:safe_control_predecessor}
 \begin{algorithmic}[1]
 \Function{SPre$_\alpha$}{$\var{G} = (V, E, r, (V_0, V_1)$, $\var{U}$, $\var{Z}_\var{outside}, \var{I}, \var{W}$}
   \State $\var{P} := \textsf{chained\_pre}(\var{G}, \var{U}, V_\alpha \cap \var{W})$
   \State $\var{P}_\alpha := P \cap V_\alpha$
   \State $\var{P}_\var{forced} := (P \cap V_{1 - \alpha}) \setminus I$
   
   \For {$1 \leq i \leq n$}
     \State $\var{P}_\var{forced} := \var{P}_\var{forced} \setminus \textsf{pre}_i(\var{P}_\textit{forced}, \var{Z}_\var{outside})$  
   \EndFor
   
   \State \Return $\var{P}_\alpha \cup \var{P}_\var{forced}$
 \EndFunction
 \end{algorithmic}
 \end{algorithm}

\subsubsection{Winning Cycle Detection}

First, we implement cycle detection for a set of vertices $V$ by determining the largest subset $U$ of $V$ such that every vertex in $U$ has a predecessor in $U$, as presented in Algorithm~\ref{alg:cycles}.
If $U$ satisfies this condition then every vertex in $U$ is part of a cycle.

\begin{algorithm}[h]
\caption{Cycle detection}
\label{alg:cycles}
\begin{algorithmic}[1]
\Function{detect-cycles}{$V$}
\State $U := \emptyset$
\State $U' := V$
\While {$U \neq U'$}
  \State $U := U'$
  \State $U' := U \cap \textsf{pred}(U, U)$
\EndWhile
\State \Return $U$
\EndFunction
\end{algorithmic}
\end{algorithm}

If one of the players can force a play through a cycle of its own priority then these vertices (and all vertices in a attractor to that set) are won by that player.
Therefore, for every priority $p$ let $\alpha = p \mod 2$ be a player and let $P = \{v \in V_\alpha \mid r(v) = p\}$ be a subset of vertices owned by player of parity $p$.
Then the set of vertices resulting from $\attr{\alpha}{\textsf{detect-cycles}(V), V}$ are won by player $\alpha$.

\newpage
\section{LDD Operations}

\newcommand{\lddnode}{\textsf{node}}
\newcommand{\lddright}{\textit{right}}
\newcommand{\ldddown}{\textit{down}}
\newcommand{\lddval}{\textit{val}}
\newcommand{\lddtrue}{\textsf{true}}
\newcommand{\lddfalse}{\textsf{false}}

For an LDD $A$ we use $\ldddown(A)$ to denote $A[x_i = v]$ and $\lddright(A)$ to denote $A[x_i > v]$.
We use $|A|$ to denote the size of the LDD, determined by the number of nodes.
Given a vector $v = x_0\,x_1 \cdots x_n$ we define its length, denoted by $|v|$, as $n + 1$.
Note that an LDD can represent (some) sets where two vectors have different lengths.
For example the set $\{1\,1, 0\}$ can be represented by an LDD.
However, the set $\{1\,1, 1\}$ cannot be represented since the root node can either have $\textsf{true}$ or $\var{node}(1, \textsf{true}, \textsf{false})$ as the down node and $\textsf{true}$ has no down or right nodes.
In general, we cannot represent a set with vectors that are strict prefixes (any vector $x_0 \cdots x_m$ with $m < n$ is a strict prefix of $v$) of other vectors in the set.
In practice, this means that we only consider LDDs where every vector in the represented set has the same length.
We refer to this length as the height of the LDD.
We often require that the input LDDs have equal height since not every output can be represented.
For example the union of $\{1\,1\}$ and $\{1\}$ cannot be represented as previously shown.

\subsection{Union}

We define the union operator on two equal height LDDs $A$ and $B$.
This computes the union of the represented sets of vectors.

\begin{algorithm}[h]
\caption{Union of two equal height LDDs $\var{A}$ and $\var{B}$}
\begin{algorithmic}[1]
\Function{union}{$A, B$}
\If{$A = B$}
	\State \Return $a$
\ElsIf{$A = \textsf{false}$}
	\State \Return $b$
\ElsIf{$B = \textsf{false}$}
	\State \Return $a$
\EndIf

\If{$val(A) < val(B)$}
	\State \Return $node(val(A), down(A), \textsc{union}(right(A), B))$
\ElsIf{$val(A) = val(B)$}
	\State \Return $node(val(A), \textsc{union}(down(A), down(B)), \textsc{union}(\lddright(A), right(B)))$
\ElsIf{$val(A) > val(B)$}
	\State \Return $node(val(B), down(B), \textsc{union}(A, right(B)))$	
\EndIf

\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{lemma}
	For all LDDs $A$ and $B$ it holds that $\interpret{\textsc{union}(A, B)} = \interpret{A} \cup \interpret{B}$.
\end{lemma}
\begin{proof}
	Pick arbitrary LDDs $A$ and $B$.
	Proof by induction on the structure of $A$ and $B$.	
	For all LDDs $A'$ and $B'$ we assume that $\interpret{\textsc{union}(A', right(B'))} = \interpret{A'} \cup \interpret{right(B')}$ and $\interpret{\textsc{union}(A', down(B'))} = \interpret{A'} \cup \interpret{down(B')}$.
	Furthermore, $\interpret{\textsc{union}(\var{right}(A'), B')} = \interpret{\var{right}(A')} \cup \interpret{B'}$ and $\interpret{\textsc{union}(down(A'), B')} = \interpret{\var{down}(A')} \cup \interpret{B'}$.
	
	Base case.
	The LDD $A$ is either $\textsf{true}$ or $\textsf{false}$.
	Then $B$ is either $\textsf{true}$ or $\textsf{false}$ due to the equal height assumption.
	In both cases t he terminal conditions ensure correctness.
	For example $\interpret{\textsc{union}(\textsf{true}, \textsf{false})} = \interpret{\textsf{true}}$ and $\{[]\} \cup \emptyset = \{[]\}$.	
	Similarly, for the case where $B$ is either $\textsf{true}$ or $\textsf{false}$.
	
	Inductive step.
	\begin{itemize}
	\item Case $val(A) < val(B)$.
		Since $A$ is an LDD we know that $\lddval(A) < \lddval(\lddright(A))$.
		Therefore, we know that $\interpret{node(val(A), down(A), \textsc{union}(right(A), B))}$ is equal to $\{val(A)\,x \mid x \in \interpret{down(A)}\} \cup \interpret{\textsc{union}(right(A), B)}$.
		It follows that $\interpret{\textsc{union}(right(A), B)}$ is equal to $\interpret{right(A)} \cup \interpret{B}$.
		From which we can derive $\interpret{A} \cup \interpret{B}$.

	\item Case $val(A) = val(B)$.
		Since $A$ is an LDD we know that $\lddval(A) < \lddval(\lddright(A))$ and similarly because $B$ is an LDD we know that $\lddval(A) < \lddval(\lddright(B))$.
		Therefore, the following node is valid and $\interpret{node(val(A), \textsc{union}(down(A), down(B)), \textsc{union}(right(A), \var{right}(B)))}$ is equal to  $\{val(A)\,x \mid x \in \interpret{\textsc{union}(down(A), down(B))}\} \cup \interpret{\textsc{union}(right(A), right(B))}$.
		It follows that the interpretation $\{val(A)\,x \mid x \in \interpret{\textsc{union}(down(A), down(B))}$ is equal to $\{val(A)\,x \mid x \in \interpret{down(A)}\} \cup \{val(A)\,x \mid x \in \interpret{down(B)}\}$ and $\interpret{\textsc{union}(right(A), right(B))}$ is equal to $\interpret{(right(A)} \cup \interpret{right(B))}$.
%		From which we can derive $\interpret{right(A)} \cup \interpret{right(B)}$.
	
	\item Case $val(A) > val(B)$.
		Similar to the $val(A) < val(B)$ case.	\qedhere	
	\end{itemize} 
\end{proof}

We can show that $|\textsc{union(A, B)}|$ for any LDDs $A$ and $B$ is at most $|A| + |B|$.
The time complexity of $\textsc{union(A, B)}$ is also of order $\mathcal{O}(|A| + |B|)$.

\subsection{Project}

Given a vector $x_0\,\cdots\,x_n$ and a subset $I \subseteq \mathbb{N}$, we define the \emph{projection}, denoted by $\textit{project}(x_0\,\cdots\,x_n, I)$, as the vector $x_{i_0}, \ldots, x_{i_l}$ for the largest $l \in \mathbb{N}$ such that $i_0 < i_1 < \ldots < i_l \leq n$ and $i_k \in I$ for $0 \leq k \leq l$.
We define the projection operator for an LDD where every vector in the set is projected.
For the LDD operator it is more convenient to specify the indices $I \subseteq \mathbb{N}$ by a vector $x_0\,\cdots\,x_n$ such that for $0 \leq i \leq n$ variable $x_i$ is one iff $i \in I$.
The operator takes an LDD $A$ of height $n$ and a sequence of numbers $x_0\,x_1,\cdots\,x_n$.

\begin{algorithm}[h]
\caption{Project vectors of an LDD $\var{A}$ of height $n$ using a sequence $x_0\,\cdots\,x_n$}
\begin{algorithmic}[1]
\Function{project}{$A, x_0\,x_1\,\cdots\,x_n$}
	\If{$A = \lddtrue$}
		\State \Return $\lddtrue$
	\ElsIf{$A = \lddfalse$}
		\State \Return $\lddfalse$
	\EndIf
	
	\If{$x_0 = 1$}
		\State \Return{$\lddnode(\lddval(A), \textsc{project}(\ldddown(A), x_1\,\cdots\,x_n), \textsc{project}(\lddright(A), x_0\,\cdots\,x_n))$}
	\ElsIf{$x_0 = 0$}
		\State $a \gets A$
		\State $R \gets \lddfalse$
		\While {$a \neq \lddfalse$}
			\State $R \gets \textsc{union}(R, \textsc{project}(\ldddown(a), x_1\,\cdots\,x_n))$
			\State $a \gets \lddright(a)$		
		\EndWhile			
		\State \Return {$R$}
	\EndIf

\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{lemma}
	For all LDDs $A$ and sequences $x_0\,x_1\,\cdots\,x_n $it holds that $\interpret{\textsc{project}(A, x_0\,\cdots\,x_n)}$ is equal to $\{\textit{project}(v, x_0\,\cdots\,x_n) \in \interpret{A}\}$.
\end{lemma}

Note that for a sequence of $|A|$ zeroes $\textsc{project}(A, 0\,\cdots\,0)$ is equal to $\lddtrue$ and for a sequence of $|A|$ ones $\textsc{project}(A, 1\,\cdots\,1)$ is equal to $A$.

\subsection{Caching}

We can speed up LDD operations at the cost of memory by using an operation cache.
For every operation there will be a separate \emph{global} cache, represented by a set, that stores a tuple of the inputs and the output.
We use \emph{global} to emphasize that every recursion sees the latest values stored in the cache.  
For example the $\textsc{union}(A, B)$ operation has a cache $C_\textsc{union}$ and the pseudocode of $\textsc{union}$ is changed such that after the terminal cases first we check whether $\exists R : (A, B, R) \in C_\textsc{union}$ and return the result $R$ if that is the case.
Otherwise, we perform the computation as before but store the result in $C_\textsc{union}$ instead before returning it.
Thus we obtain the following algorithm:

\begin{algorithm}[H]
\caption{Union of two LDDs $\var{A}$ and $\var{B}$.
	With a set $C_\textsc{union}$ as operation cache}
\begin{algorithmic}[1]
\Function{union}{$A, B,  C_\textsc{union}$}
\If{$A = B$}
	\State \Return $a$
\ElsIf{$A = \textsf{false}$}
	\State \Return $b$
\ElsIf{$B = \textsf{false}$}
	\State \Return $a$
\EndIf

\If{$\exists R: (A, B, R) \in C_\textsc{union}$}
	\Return $R$
\EndIf

\If{$val(A) < val(B)$}
	\State $R \gets node(val(A), down(A), \textsc{union}(right(A), B, C_\textsc{union}))$
\ElsIf{$val(A) = val(B)$}
	\State $R \gets node(val(A), union(down(A), down(B), union(right(A), right(B), C_\textsc{union}))$
\ElsIf{$val(A) > val(B)$}
	\State $R \gets node(val(B), down(B), \textsc{union}(A, right(B), C_\textsc{union}))$
\EndIf

\State $C_\textsc{union} \gets C_\textsc{union} \cup \{(A, B, R)\}$
\State \Return $R$
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{Parallelism}

In some cases we can improve the performance further by computing several results in parallel during the operation.
For example in the union of two LDDs $A$ and $B$ in the case $val(A) = val(B)$ we could determine the result of both unions in parallel and only merge them after they have finished.

\bibliographystyle{plain}
\bibliography{symbolic-reachability}
\end{document}
